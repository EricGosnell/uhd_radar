{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "client = Client(n_workers=1,\n",
    "                threads_per_worker=4,\n",
    "                memory_limit='15GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import time\n",
    "import os\n",
    "\n",
    "import dask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import scipy.constants\n",
    "import scipy\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import processing_dask as pr\n",
    "import plot_dask\n",
    "\n",
    "sys.path.append(\"../../../preprocessing/\")\n",
    "from generate_chirp import generate_chirp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"/media/thomas/Extreme SSD/orca_paper_data_files/phase_noise/b205/20240222_203345\"\n",
    "\n",
    "zero_sample_idx = 159\n",
    "sig_speed = scipy.constants.speed_of_light * (2/3)\n",
    "\n",
    "zarr_path = pr.save_radar_data_to_zarr(prefix, zarr_base_location=\"/home/thomas/Documents/StanfordGrad/RadioGlaciology/test_tmp_zarr_cache/\", skip_if_cached=True)\n",
    "\n",
    "zarr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = xr.open_zarr(zarr_path)\n",
    "raw = raw[{'pulse_idx': slice(0, 50000)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirp_ts, chirp = generate_chirp(raw.config)\n",
    "\n",
    "compressed = pr.pulse_compress(raw, chirp,\n",
    "                               fs=raw.config['GENERATE']['sample_rate'],\n",
    "                               zero_sample_idx=zero_sample_idx,\n",
    "                               signal_speed=sig_speed).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.logspace(np.log10(250e-6), np.log10(2.5*60), 10)\n",
    "#ts = np.logspace(np.log10(2e-2), np.log10(300), 20)\n",
    "#ts = np.logspace(np.log10(2e-2), np.log10(1000), 20)\n",
    "\n",
    "expected_reflector_distance_1way = 50 # m\n",
    "reflector_peak_tol_bins = 2 # bins (on each side)\n",
    "noise_start_distance_1way = 1000 # m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Floor Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_stack_t = np.nan * np.zeros_like(ts)\n",
    "actual_stack_n = np.zeros_like(ts, dtype=int)\n",
    "\n",
    "# Statistics to compute\n",
    "stack_noise_var = np.nan * np.zeros_like(ts)\n",
    "stack_noise_mean = np.nan * np.zeros_like(ts)\n",
    "stack_signal_peak_pwr_mean = np.nan * np.zeros_like(ts)\n",
    "stack_signal_peak_pwr_variance = np.nan * np.zeros_like(ts)\n",
    "stack_signal_peak_phase = np.nan * np.zeros_like(ts)\n",
    "stack_signal_peak_phase_variance = np.nan * np.zeros_like(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for t_idx, t in enumerate(ts):\n",
    "    if not np.isnan(stack_noise_mean[t_idx]):\n",
    "        continue # Skip if already computed (in case of interruption and restart)\n",
    "    \n",
    "    timestamp = time.time() # Track computation time \n",
    "\n",
    "    actual_stack_n[t_idx] = max(1, int(t / raw.attrs['config']['CHIRP']['pulse_rep_int']))\n",
    "    actual_stack_t[t_idx] = actual_stack_n[t_idx] * raw.attrs['config']['CHIRP']['pulse_rep_int'] # TODO: Account for errors?\n",
    "    print(f\"[{t_idx+1}/{len(ts)}] \\tt={actual_stack_t[t_idx]} \\tn_stack={actual_stack_n[t_idx]}\")\n",
    "    \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "\n",
    "        stacked = pr.stack(compressed, actual_stack_n[t_idx])\n",
    "        compressed_pwr = xr.apply_ufunc(lambda x: np.abs(x)**2, stacked, dask='parallelized').chunk(\"auto\")\n",
    "        \n",
    "        noise_pwr = compressed_pwr[\"radar_data\"].where((compressed_pwr.reflection_distance > noise_start_distance_1way)).dropna('travel_time').chunk(\"auto\")\n",
    "        \n",
    "        stack_noise_var[t_idx] = noise_pwr.var(dim=\"travel_time\").mean().compute().item()\n",
    "        stack_noise_mean[t_idx] = noise_pwr.mean().compute().item()\n",
    "\n",
    "        # Compute the signal peak power and phase\n",
    "        \n",
    "        expected_peak_idx = (np.abs(stacked.reflection_distance - expected_reflector_distance_1way)).argmin().item()\n",
    "\n",
    "        peak_idxs = stacked[\"radar_data\"].reduce(\n",
    "            lambda x, axis: (np.abs((x[:, expected_peak_idx-reflector_peak_tol_bins:expected_peak_idx+reflector_peak_tol_bins]))\n",
    "                             ).argmax(axis=axis) + expected_peak_idx-reflector_peak_tol_bins, dim='travel_time').persist()\n",
    "        \n",
    "        true_peak_idx = peak_idxs[0].compute().item()\n",
    "        if not (peak_idxs == true_peak_idx).all().compute().item():\n",
    "            print(\"WARNING: Peak indices are not all the same!\")\n",
    "\n",
    "        peak_phases = xr.apply_ufunc(\n",
    "            lambda x, idx: np.angle(x[idx]),\n",
    "            stacked[\"radar_data\"], peak_idxs,\n",
    "            input_core_dims=[['travel_time'],[]], # The dimension operated over -- aka \"don't vectorize over this\"\n",
    "            output_core_dims=[[]], # The output dimensions of the lambda function itself\n",
    "            exclude_dims=set((\"travel_time\",)), # Dimensions to not vectorize over\n",
    "            vectorize=True, # Vectorize other dimensions using a call to np.vectorize\n",
    "            dask=\"parallelized\", # Allow dask to chunk and parallelize the computation\n",
    "            output_dtypes=[np.float32], # Needed for dask: explicitly provide the output dtype\n",
    "            #dask_gufunc_kwargs={\"output_sizes\": {'travel_time': 1}} # Also needed for dask:\n",
    "            # explicitly provide the output size of the lambda function. See\n",
    "            # https://docs.dask.org/en/stable/generated/dask.array.gufunc.apply_gufunc.html\n",
    "        ).persist()\n",
    "\n",
    "        peak_pwr = xr.apply_ufunc(\n",
    "            lambda x, idx: np.abs(x[idx])**2,\n",
    "            stacked[\"radar_data\"], peak_idxs,\n",
    "            input_core_dims=[['travel_time'],[]], # The dimension operated over -- aka \"don't vectorize over this\"\n",
    "            output_core_dims=[[]], # The output dimensions of the lambda function itself\n",
    "            exclude_dims=set((\"travel_time\",)), # Dimensions to not vectorize over\n",
    "            vectorize=True, # Vectorize other dimensions using a call to np.vectorize\n",
    "            dask=\"parallelized\", # Allow dask to chunk and parallelize the computation\n",
    "            output_dtypes=[np.float32], # Needed for dask: explicitly provide the output dtype\n",
    "            #dask_gufunc_kwargs={\"output_sizes\": {'travel_time': 1}} # Also needed for dask:\n",
    "            # explicitly provide the output size of the lambda function. See\n",
    "            # https://docs.dask.org/en/stable/generated/dask.array.gufunc.apply_gufunc.html\n",
    "        ).persist()\n",
    "\n",
    "        stack_signal_peak_pwr_mean[t_idx] = peak_pwr.mean().compute().item()\n",
    "        stack_signal_peak_pwr_variance[t_idx] = peak_pwr.var().compute().item()\n",
    "        stack_signal_peak_phase[t_idx] = peak_phases.mean().compute().item()\n",
    "        stack_signal_peak_phase_variance[t_idx] = peak_phases.var().compute().item()\n",
    "        \n",
    "    \n",
    "    print(f\"Completed in {time.time() - timestamp} seconds from {len(noise_pwr)} stacked pulses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_noise_var, ax_noise_mean, ax_signal_mag, ax_signal_ph, ax_signal_ph_var) = plt.subplots(5, 1, figsize=(10, 20))\n",
    "\n",
    "ax_noise_var.scatter(actual_stack_n, stack_noise_var)\n",
    "ax_noise_var.set_title(\"Noise Variance\")\n",
    "ax_noise_var.loglog()\n",
    "ax_noise_var.grid()\n",
    "\n",
    "ax_noise_mean.scatter(actual_stack_n, stack_noise_mean)\n",
    "ax_noise_mean.set_title(\"Noise Mean\")\n",
    "ax_noise_mean.loglog()\n",
    "ax_noise_mean.grid()\n",
    "\n",
    "ax_signal_mag.scatter(actual_stack_n, stack_signal_peak_pwr)\n",
    "ax_signal_mag.set_title(\"Signal Magnitude\")\n",
    "ax_signal_mag.loglog()\n",
    "ax_signal_mag.grid()\n",
    "\n",
    "ax_signal_ph.scatter(actual_stack_n, stack_signal_peak_phase)\n",
    "ax_signal_ph.set_title(\"Signal Phase\")\n",
    "ax_signal_ph.semilogx()\n",
    "ax_signal_ph.grid()\n",
    "\n",
    "ax_signal_ph_var.scatter(actual_stack_n, stack_signal_peak_phase_variance)\n",
    "ax_signal_ph_var.set_title(\"Signal Phase Variance\")\n",
    "ax_signal_ph_var.loglog()\n",
    "ax_signal_ph_var.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "ax_pwr, ax_ph = axs\n",
    "\n",
    "ax_pwr.plot(actual_stack_t, stack_signal_peak_pwr_mean)\n",
    "# Add a shaded region for the variance\n",
    "ax_pwr.fill_between(actual_stack_t, stack_signal_peak_pwr_mean - np.sqrt(stack_signal_peak_pwr_variance),\n",
    "                    stack_signal_peak_pwr_mean + np.sqrt(stack_signal_peak_pwr_variance), alpha=0.4)\n",
    "ax_pwr.set_title(\"Signal Power\")\n",
    "ax_pwr.loglog()\n",
    "\n",
    "ax_ph.plot(actual_stack_t, stack_signal_peak_phase)\n",
    "# Add a shaded region for the variance\n",
    "ax_ph.fill_between(actual_stack_t, stack_signal_peak_phase - np.sqrt(stack_signal_peak_phase_variance),\n",
    "                    stack_signal_peak_phase + np.sqrt(stack_signal_peak_phase_variance), alpha=0.4)\n",
    "ax_ph.set_title(\"Signal Phase\")\n",
    "ax_ph.semilogx()\n",
    "\n",
    "for ax in axs:\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog()\n",
    "ax.scatter(actual_stack_n, stack_noise_var)\n",
    "ax.set_xlabel('n_stack')\n",
    "ax.set_ylabel('Variance of noise floor (2-4km)')\n",
    "ax.set_title(f\"pulse_rep_int = {raw.attrs['config']['CHIRP']['pulse_rep_int']} s\")\n",
    "plt.grid()\n",
    "#fig.savefig(output_base_stack + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal peak phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal\n",
    "reflector_distance_expected = 25\n",
    "expected_peak_idx = (np.abs(compressed.reflection_distance - reflector_distance_expected)).argmin().item()\n",
    "\n",
    "peak_idxs = compressed[\"radar_data\"].reduce(\n",
    "    lambda x, axis: (np.abs((x[:, expected_peak_idx-5:expected_peak_idx+5]))).argmax(axis=axis) + expected_peak_idx-5,\n",
    "    dim='travel_time')\n",
    "peak_idxs.persist()\n",
    "true_peak_idx = peak_idxs[0].compute().item()\n",
    "if not (peak_idxs == true_peak_idx).all().compute().item():\n",
    "    print(\"WARNING: Peak indices are not all the same!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_internal_path_idx = (np.abs(compressed.reflection_distance)).argmin().item()\n",
    "expected_internal_path_idx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_stack = 1000\n",
    "internal_path_phases_stacked = xr.apply_ufunc(\n",
    "        lambda x: np.angle(x[expected_internal_path_idx]),\n",
    "        pr.stack(compressed, 100)[\"radar_data\"],\n",
    "        input_core_dims=[['travel_time']], # The dimension operated over -- aka \"don't vectorize over this\"\n",
    "        output_core_dims=[[]], # The output dimensions of the lambda function itself\n",
    "        exclude_dims=set((\"travel_time\",)), # Dimensions to not vectorize over\n",
    "        vectorize=True, # Vectorize other dimensions using a call to np.vectorize\n",
    "        dask=\"parallelized\", # Allow dask to chunk and parallelize the computation\n",
    "        output_dtypes=[np.float32], # Needed for dask: explicitly provide the output dtype\n",
    "        #dask_gufunc_kwargs={\"output_sizes\": {'travel_time': 1}} # Also needed for dask:\n",
    "        # explicitly provide the output size of the lambda function. See\n",
    "        # https://docs.dask.org/en/stable/generated/dask.array.gufunc.apply_gufunc.html\n",
    "    ).persist()\n",
    "loopback_path_phases_stacked = xr.apply_ufunc(\n",
    "        lambda x: np.angle(x[expected_peak_idx]),\n",
    "        pr.stack(compressed, 100)[\"radar_data\"],\n",
    "        input_core_dims=[['travel_time']], # The dimension operated over -- aka \"don't vectorize over this\"\n",
    "        output_core_dims=[[]], # The output dimensions of the lambda function itself\n",
    "        exclude_dims=set((\"travel_time\",)), # Dimensions to not vectorize over\n",
    "        vectorize=True, # Vectorize other dimensions using a call to np.vectorize\n",
    "        dask=\"parallelized\", # Allow dask to chunk and parallelize the computation\n",
    "        output_dtypes=[np.float32], # Needed for dask: explicitly provide the output dtype\n",
    "        #dask_gufunc_kwargs={\"output_sizes\": {'travel_time': 1}} # Also needed for dask:\n",
    "        # explicitly provide the output size of the lambda function. See\n",
    "        # https://docs.dask.org/en/stable/generated/dask.array.gufunc.apply_gufunc.html\n",
    "    ).persist()\n",
    "\n",
    "plt.scatter(internal_path_phases_stacked, loopback_path_phases_stacked, alpha=0.1)\n",
    "#plt.ylim(-3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_phases = xr.apply_ufunc(\n",
    "        lambda x, idx: np.angle(x[idx]),\n",
    "        compressed[\"radar_data\"], peak_idxs,\n",
    "        input_core_dims=[['travel_time'],[]], # The dimension operated over -- aka \"don't vectorize over this\"\n",
    "        output_core_dims=[[]], # The output dimensions of the lambda function itself\n",
    "        exclude_dims=set((\"travel_time\",)), # Dimensions to not vectorize over\n",
    "        vectorize=True, # Vectorize other dimensions using a call to np.vectorize\n",
    "        dask=\"parallelized\", # Allow dask to chunk and parallelize the computation\n",
    "        output_dtypes=[np.float32], # Needed for dask: explicitly provide the output dtype\n",
    "        #dask_gufunc_kwargs={\"output_sizes\": {'travel_time': 1}} # Also needed for dask:\n",
    "        # explicitly provide the output size of the lambda function. See\n",
    "        # https://docs.dask.org/en/stable/generated/dask.array.gufunc.apply_gufunc.html\n",
    "    ).persist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO -- debugging\n",
    "\n",
    "# fs = raw.attrs['config']['GENERATE']['sample_rate']\n",
    "\n",
    "# ts = np.logspace(np.log10(2e-2), np.log10(200), 30)\n",
    "# actual_dt = np.zeros_like(ts)\n",
    "# var = np.zeros_like(ts)\n",
    "\n",
    "t_idx = 2\n",
    "t = ts[t_idx]\n",
    "\n",
    "\n",
    "pulses = int(t / raw.attrs['config']['CHIRP']['pulse_rep_int'])\n",
    "actual_dt[t_idx] = pulses * raw.attrs['config']['CHIRP']['pulse_rep_int']\n",
    "ph_group_mean = peak_phases.rolling(pulse_idx=pulses).mean()\n",
    "var[t_idx] = ((ph_group_mean[:-pulses].drop_indexes(\"pulse_idx\") - ph_group_mean[pulses:].drop_indexes(\"pulse_idx\"))**2).mean().compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs = raw.attrs['config']['GENERATE']['sample_rate']\n",
    "\n",
    "actual_dt = np.zeros_like(ts)\n",
    "var = np.zeros_like(ts)\n",
    "\n",
    "for t_idx, t in enumerate(ts):\n",
    "    print(f\"[{t_idx}/{len(ts)}] \\tt={t}\")\n",
    "    pulses = max(1, int(t / raw.attrs['config']['CHIRP']['pulse_rep_int']))\n",
    "    actual_dt[t_idx] = pulses * raw.attrs['config']['CHIRP']['pulse_rep_int']\n",
    "    ph_group_mean = peak_phases.rolling(pulse_idx=pulses).mean()\n",
    "    var[t_idx] = ((ph_group_mean[:-pulses].drop_indexes(\"pulse_idx\") - ph_group_mean[pulses:].drop_indexes(\"pulse_idx\"))**2).mean().compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_2svar = os.path.join(\"20230628-outputs/\", raw.attrs[\"basename\"]+\"-2svar\")\n",
    "\n",
    "d = xr.Dataset({\"var_2s\": (\"dt\", var)}, coords={\"dt\": actual_dt})\n",
    "d.to_netcdf(output_base_2svar + \".nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog()\n",
    "ax.scatter(actual_dt, var)\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('Two sample phase variance')\n",
    "ax.set_title(f\"pulse_rep_int = {raw.attrs['config']['CHIRP']['pulse_rep_int']} s\")\n",
    "plt.grid()\n",
    "fig.savefig(output_base_2svar + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_phase = os.path.join(\"20230628-outputs/\", raw.attrs[\"basename\"]+\"-phase\")\n",
    "\n",
    "peak_idx_plot = peak_idxs.hvplot.scatter(x='pulse_idx')\n",
    "peak_phase_plot = peak_phases.hvplot.scatter(x='pulse_idx', datashade=True)\n",
    "peak_phase_rolling_plot = peak_phases.rolling(pulse_idx=100).mean().hvplot.scatter(x='pulse_idx', datashade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.save(peak_idx_plot, output_base_phase+\"-peak-idx.png\", fmt='png')\n",
    "hv.save(peak_phase_plot, output_base_phase+\"-peak-phase.png\", fmt='png')\n",
    "hv.save(peak_phase_rolling_plot, output_base_phase+\"-peak-phase-rolling.png\", fmt='png')\n",
    "\n",
    "hv.save(peak_idx_plot, output_base_phase+\"-peak-idx.html\", fmt='widgets')\n",
    "hv.save(peak_phase_plot, output_base_phase+\"-peak-phase.html\", fmt='widgets')\n",
    "hv.save(peak_phase_rolling_plot, output_base_phase+\"-peak-phase-rolling.html\", fmt='widgets')\n",
    "\n",
    "peak_idx_plot, peak_phase_plot, peak_phase_rolling_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
