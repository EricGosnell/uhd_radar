{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "client = Client(n_workers=1,\n",
    "                threads_per_worker=7,\n",
    "                memory_limit='18GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import time\n",
    "import os\n",
    "\n",
    "import dask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import scipy.constants\n",
    "import scipy\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import processing_dask as pr\n",
    "import plot_dask\n",
    "\n",
    "sys.path.append(\"../../preprocessing/\")\n",
    "from generate_chirp import generate_chirp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = \"/home/thomas/Documents/StanfordGrad/RadioGlaciology/sdr/data/20230711_115449\"\n",
    "\n",
    "# For these: 50 m loopback cable, 30 dB attenuation before the cable, lab SDR\n",
    "#prefix = \"/home/thomas/Documents/StanfordGrad/RadioGlaciology/sdr/data/20231206_170356\" # 100k pulses, seemed like this one was maybe leveling out?\n",
    "#prefix = \"/home/thomas/Documents/StanfordGrad/RadioGlaciology/sdr/data/20231206_173558\" # 1 M pulses, 10 MHz BW, 10 us chirp duration\n",
    "#prefix = \"/home/thomas/Documents/StanfordGrad/RadioGlaciology/sdr/data/20231206_174958\" # 1 M pulses, 40 MHz BW, 10 us chirp duration\n",
    "#prefix = \"/home/thomas/Documents/StanfordGrad/RadioGlaciology/sdr/data/20231209_150916\" # 1 M pulses, 40 MHz BW, 10 us chirp duration (same setup, different day)\n",
    "# 150 m of loopback cable, 0 dB attenuation before the cable, lab SDR\n",
    "prefix = \"/home/thomas/Documents/StanfordGrad/RadioGlaciology/sdr/data/20231209_151613\" # 1 M pulses, 40 MHz BW, 10 us chirp duration (150 m of cable)\n",
    "\n",
    "zero_sample_idx = 159\n",
    "sig_speed = scipy.constants.speed_of_light * (2/3)\n",
    "\n",
    "zarr_path = pr.save_radar_data_to_zarr(prefix, zarr_base_location=\"/home/thomas/Documents/StanfordGrad/RadioGlaciology/test_tmp_zarr_cache/\", skip_if_cached=True)\n",
    "\n",
    "zarr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = xr.open_zarr(zarr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = copy.deepcopy(raw.config)\n",
    "#config['GENERATE']['window'] = 'blackman'\n",
    "\n",
    "chirp_ts, chirp = generate_chirp(config)\n",
    "\n",
    "# # Filter chirp\n",
    "# chirp_freq_sweep_mhz = np.linspace(-28, 28, len(chirp))\n",
    "# keep_mask = (chirp_freq_sweep_mhz > -10) & (chirp_freq_sweep_mhz < -1)\n",
    "# chirp_filtered = np.zeros_like(chirp)\n",
    "# chirp_filtered[keep_mask] = chirp[keep_mask]\n",
    "# chirp = chirp_filtered\n",
    "# # / Filter chirp\n",
    "\n",
    "compressed = pr.pulse_compress(raw, chirp,\n",
    "                               fs=raw.config['GENERATE']['sample_rate'],\n",
    "                               zero_sample_idx=zero_sample_idx,\n",
    "                               signal_speed=scipy.constants.c * (2/3)).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.logspace(np.log10(2e-2), np.log10(300), 10)\n",
    "ts = np.logspace(np.log10(2e-2), np.log10(10), 10)\n",
    "#ts = np.logspace(np.log10(2e-2), np.log10(300), 20)\n",
    "#ts = np.logspace(np.log10(2e-2), np.log10(1000), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Floor Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_stack_t = np.nan * np.zeros_like(ts)\n",
    "actual_stack_n = np.zeros_like(ts, dtype=int)\n",
    "stack_noise_var = np.nan * np.zeros_like(ts)\n",
    "stack_noise_mean = np.nan * np.zeros_like(ts)\n",
    "stack_signal_mean = np.nan * np.zeros_like(ts)\n",
    "stack_signal_var = np.nan * np.zeros_like(ts)\n",
    "\n",
    "noise_start_m = 2000\n",
    "noise_end_m = 4000\n",
    "signal_start_m = 70\n",
    "signal_end_m = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for t_idx, t in enumerate(ts):\n",
    "    if not np.isnan(stack_noise_mean[t_idx]):\n",
    "        continue\n",
    "    \n",
    "    timestamp = time.time()\n",
    "    actual_stack_n[t_idx] = max(1, int(t / raw.attrs['config']['CHIRP']['pulse_rep_int']))\n",
    "    actual_stack_t[t_idx] = actual_stack_n[t_idx] * raw.attrs['config']['CHIRP']['pulse_rep_int']\n",
    "    print(f\"[{t_idx+1}/{len(ts)}] \\tt={actual_stack_t[t_idx]} \\tn_stack={actual_stack_n[t_idx]}\")\n",
    "    \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        compressed_subset = compressed[{'pulse_idx': slice(0, actual_stack_n[t_idx]*100)}]\n",
    "        stacked = pr.stack(compressed_subset, actual_stack_n[t_idx])\n",
    "        compressed_mag = xr.apply_ufunc(np.abs, stacked, dask='parallelized').chunk(\"auto\")\n",
    "        \n",
    "        # Noise floor\n",
    "        vs = compressed_mag[\"radar_data\"].where((compressed_mag.reflection_distance > noise_start_m) & (compressed_mag.reflection_distance < noise_end_m)).dropna('travel_time').chunk(\"auto\")\n",
    "        if len(vs) > 20:\n",
    "            vs = vs[:20]\n",
    "        stack_noise_var[t_idx] = vs.var(dim=\"travel_time\").mean().compute().item()\n",
    "        stack_noise_mean[t_idx] = vs.mean().compute().item()\n",
    "\n",
    "        # Signal peak\n",
    "        ss = compressed_mag[\"radar_data\"].where((compressed_mag.reflection_distance > signal_start_m) & (compressed_mag.reflection_distance < signal_end_m)).dropna('travel_time').chunk(\"auto\")\n",
    "        if len(ss) > 100:\n",
    "            ss = ss[:100]\n",
    "        stack_signal_mean[t_idx] = ss.max(dim=\"travel_time\").mean().compute().item()\n",
    "        stack_signal_var[t_idx] = ss.max(dim=\"travel_time\").var().compute().item()\n",
    "        \n",
    "    print(f\"Completed in {time.time() - timestamp} seconds from {len(vs)} computed variances and {len(ss)} computed signal peaks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_base_stack = os.path.join(\"20230628-outputs/\", raw.attrs[\"basename\"]+\"-stack\")\n",
    "\n",
    "# d = xr.Dataset({\"noise_var\": (\"t\", stack_noise_var)}, coords={\"t\": actual_stack_t, \"n_stack\": (\"t\", actual_stack_n)})\n",
    "# d.to_netcdf(output_base_stack + \".nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_stack_n[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save actual_stack_t, actual_stack_n, stack_noise_var, stack_noise_mean, stack_signal_mean, stack_signal_var to a pickle file\n",
    "import pickle\n",
    "with open(os.path.join(\"20231208-outputs/\", os.path.basename(prefix) + \"-stats.pickle\"), \"wb\") as f:\n",
    "    pickle.dump({\"actual_stack_t\": actual_stack_t,\n",
    "                 \"actual_stack_n\": actual_stack_n,\n",
    "                 \"stack_noise_var\": stack_noise_var,\n",
    "                 \"stack_noise_mean\": stack_noise_mean,\n",
    "                 \"stack_signal_mean\": stack_signal_mean,\n",
    "                 \"stack_signal_var\": stack_signal_var,\n",
    "                 \"config\": raw.attrs[\"config\"]}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogx()\n",
    "#ax.scatter(actual_stack_t, stack_signal_var, label=\"Variance\")\n",
    "ax.scatter(actual_stack_t, 20*np.log10(stack_signal_mean), label=\"Mean\")\n",
    "ax.fill_between(actual_stack_t, 20*np.log10(stack_signal_mean - np.sqrt(stack_signal_var)), 20*np.log10(stack_signal_mean + np.sqrt(stack_signal_var)), alpha=0.2)\n",
    "ax.set_xlabel('Total coherent integration time [s]')\n",
    "ax.set_ylabel('Signal peak')\n",
    "ax.set_title(f\"pulse_rep_int = {raw.attrs['config']['CHIRP']['pulse_rep_int']} s\\n{os.path.basename(prefix)}\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "ax_n = ax.twiny()\n",
    "ax_n.semilogx()\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ax_n.set_xlim(xmin / raw.attrs['config']['CHIRP']['pulse_rep_int'], xmax / raw.attrs['config']['CHIRP']['pulse_rep_int'])\n",
    "ax_n.set_xlabel('n_stack')\n",
    "ax_n_ticks = (ax.get_xticks()/raw.attrs['config']['CHIRP']['pulse_rep_int']).round(1).astype(int)\n",
    "ax_n.set_xticks([500, 5000, 50000])\n",
    "ax_n.set_xticklabels([500, 5000, 50000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_n_ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/raw.attrs['config']['CHIRP']['pulse_rep_int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog()\n",
    "ax.scatter(actual_stack_t, stack_noise_var, label=\"Variance\")\n",
    "#ax.scatter(actual_stack_t, stack_noise_mean, label=\"Mean\")\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('Noise floor (2-4km)')\n",
    "ax.set_title(f\"pulse_rep_int = {raw.attrs['config']['CHIRP']['pulse_rep_int']} s\")\n",
    "ax.legend()\n",
    "plt.grid()\n",
    "#fig.savefig(output_base_stack + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog()\n",
    "ax.scatter(actual_stack_n, stack_noise_var)\n",
    "ax.set_xlabel('n_stack')\n",
    "ax.set_ylabel('Variance of noise floor (2-4km)')\n",
    "ax.set_title(f\"pulse_rep_int = {raw.attrs['config']['CHIRP']['pulse_rep_int']} s\")\n",
    "plt.grid()\n",
    "#fig.savefig(output_base_stack + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles = [\n",
    "    \"20231208-outputs/20231206_173558-stats.pickle\",\n",
    "    \"20231208-outputs/20231206_174958-stats.pickle\",\n",
    "    \"20231208-outputs/20231209_150916-stats.pickle\",\n",
    "    \"20231208-outputs/20231209_151613-stats.pickle\",\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx()\n",
    "\n",
    "for pickle_path in pickles:\n",
    "    with open(pickle_path, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "    sig_mean_db = 20*np.log10(d[\"stack_signal_mean\"])\n",
    "    ax.scatter(d[\"actual_stack_t\"], sig_mean_db - sig_mean_db[0], label=os.path.basename(pickle_path).split(\"-\")[0])\n",
    "\n",
    "#ax.scatter(actual_stack_t, 20*np.log10(stack_signal_mean), label=\"Mean\")\n",
    "#ax.fill_between(actual_stack_t, 20*np.log10(stack_signal_mean - np.sqrt(stack_signal_var)), 20*np.log10(stack_signal_mean + np.sqrt(stack_signal_var)), alpha=0.2)\n",
    "ax.set_xlabel('Total coherent integration time [s]')\n",
    "ax.set_ylabel('Signal peak')\n",
    "ax.set_title(f\"pulse_rep_int = {raw.attrs['config']['CHIRP']['pulse_rep_int']} s\\n{os.path.basename(prefix)}\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "ax_n = ax.twiny()\n",
    "ax_n.semilogx()\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ax_n.set_xlim(xmin / raw.attrs['config']['CHIRP']['pulse_rep_int'], xmax / raw.attrs['config']['CHIRP']['pulse_rep_int'])\n",
    "ax_n.set_xlabel('n_stack')\n",
    "ax_n_ticks = (ax.get_xticks()/raw.attrs['config']['CHIRP']['pulse_rep_int']).round(1).astype(int)\n",
    "ax_n.set_xticks([500, 5000, 50000])\n",
    "ax_n.set_xticklabels([500, 5000, 50000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal peak phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal\n",
    "reflector_distance_expected = 25\n",
    "expected_peak_idx = (np.abs(compressed.reflection_distance - reflector_distance_expected)).argmin().item()\n",
    "\n",
    "peak_idxs = compressed[\"radar_data\"].reduce(\n",
    "    lambda x, axis: (np.abs((x[:, expected_peak_idx-5:expected_peak_idx+5]))).argmax(axis=axis) + expected_peak_idx-5,\n",
    "    dim='travel_time')\n",
    "peak_idxs.persist()\n",
    "true_peak_idx = peak_idxs[0].compute().item()\n",
    "if not (peak_idxs == true_peak_idx).all().compute().item():\n",
    "    print(\"WARNING: Peak indices are not all the same!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_internal_path_idx = (np.abs(compressed.reflection_distance)).argmin().item()\n",
    "expected_internal_path_idx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_stack = 1000\n",
    "internal_path_phases_stacked = xr.apply_ufunc(\n",
    "        lambda x: np.angle(x[expected_internal_path_idx]),\n",
    "        pr.stack(compressed, 100)[\"radar_data\"],\n",
    "        input_core_dims=[['travel_time']], # The dimension operated over -- aka \"don't vectorize over this\"\n",
    "        output_core_dims=[[]], # The output dimensions of the lambda function itself\n",
    "        exclude_dims=set((\"travel_time\",)), # Dimensions to not vectorize over\n",
    "        vectorize=True, # Vectorize other dimensions using a call to np.vectorize\n",
    "        dask=\"parallelized\", # Allow dask to chunk and parallelize the computation\n",
    "        output_dtypes=[np.float32], # Needed for dask: explicitly provide the output dtype\n",
    "        #dask_gufunc_kwargs={\"output_sizes\": {'travel_time': 1}} # Also needed for dask:\n",
    "        # explicitly provide the output size of the lambda function. See\n",
    "        # https://docs.dask.org/en/stable/generated/dask.array.gufunc.apply_gufunc.html\n",
    "    ).persist()\n",
    "loopback_path_phases_stacked = xr.apply_ufunc(\n",
    "        lambda x: np.angle(x[expected_peak_idx]),\n",
    "        pr.stack(compressed, 100)[\"radar_data\"],\n",
    "        input_core_dims=[['travel_time']], # The dimension operated over -- aka \"don't vectorize over this\"\n",
    "        output_core_dims=[[]], # The output dimensions of the lambda function itself\n",
    "        exclude_dims=set((\"travel_time\",)), # Dimensions to not vectorize over\n",
    "        vectorize=True, # Vectorize other dimensions using a call to np.vectorize\n",
    "        dask=\"parallelized\", # Allow dask to chunk and parallelize the computation\n",
    "        output_dtypes=[np.float32], # Needed for dask: explicitly provide the output dtype\n",
    "        #dask_gufunc_kwargs={\"output_sizes\": {'travel_time': 1}} # Also needed for dask:\n",
    "        # explicitly provide the output size of the lambda function. See\n",
    "        # https://docs.dask.org/en/stable/generated/dask.array.gufunc.apply_gufunc.html\n",
    "    ).persist()\n",
    "\n",
    "plt.scatter(internal_path_phases_stacked, loopback_path_phases_stacked, alpha=0.1)\n",
    "#plt.ylim(-3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_phases = xr.apply_ufunc(\n",
    "        lambda x, idx: np.angle(x[idx]),\n",
    "        compressed[\"radar_data\"], peak_idxs,\n",
    "        input_core_dims=[['travel_time'],[]], # The dimension operated over -- aka \"don't vectorize over this\"\n",
    "        output_core_dims=[[]], # The output dimensions of the lambda function itself\n",
    "        exclude_dims=set((\"travel_time\",)), # Dimensions to not vectorize over\n",
    "        vectorize=True, # Vectorize other dimensions using a call to np.vectorize\n",
    "        dask=\"parallelized\", # Allow dask to chunk and parallelize the computation\n",
    "        output_dtypes=[np.float32], # Needed for dask: explicitly provide the output dtype\n",
    "        #dask_gufunc_kwargs={\"output_sizes\": {'travel_time': 1}} # Also needed for dask:\n",
    "        # explicitly provide the output size of the lambda function. See\n",
    "        # https://docs.dask.org/en/stable/generated/dask.array.gufunc.apply_gufunc.html\n",
    "    ).persist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO -- debugging\n",
    "\n",
    "# fs = raw.attrs['config']['GENERATE']['sample_rate']\n",
    "\n",
    "# ts = np.logspace(np.log10(2e-2), np.log10(200), 30)\n",
    "# actual_dt = np.zeros_like(ts)\n",
    "# var = np.zeros_like(ts)\n",
    "\n",
    "t_idx = 2\n",
    "t = ts[t_idx]\n",
    "\n",
    "\n",
    "pulses = int(t / raw.attrs['config']['CHIRP']['pulse_rep_int'])\n",
    "actual_dt[t_idx] = pulses * raw.attrs['config']['CHIRP']['pulse_rep_int']\n",
    "ph_group_mean = peak_phases.rolling(pulse_idx=pulses).mean()\n",
    "var[t_idx] = ((ph_group_mean[:-pulses].drop_indexes(\"pulse_idx\") - ph_group_mean[pulses:].drop_indexes(\"pulse_idx\"))**2).mean().compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs = raw.attrs['config']['GENERATE']['sample_rate']\n",
    "\n",
    "actual_dt = np.zeros_like(ts)\n",
    "var = np.zeros_like(ts)\n",
    "\n",
    "for t_idx, t in enumerate(ts):\n",
    "    print(f\"[{t_idx}/{len(ts)}] \\tt={t}\")\n",
    "    pulses = max(1, int(t / raw.attrs['config']['CHIRP']['pulse_rep_int']))\n",
    "    actual_dt[t_idx] = pulses * raw.attrs['config']['CHIRP']['pulse_rep_int']\n",
    "    ph_group_mean = peak_phases.rolling(pulse_idx=pulses).mean()\n",
    "    var[t_idx] = ((ph_group_mean[:-pulses].drop_indexes(\"pulse_idx\") - ph_group_mean[pulses:].drop_indexes(\"pulse_idx\"))**2).mean().compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_2svar = os.path.join(\"20230628-outputs/\", raw.attrs[\"basename\"]+\"-2svar\")\n",
    "\n",
    "d = xr.Dataset({\"var_2s\": (\"dt\", var)}, coords={\"dt\": actual_dt})\n",
    "d.to_netcdf(output_base_2svar + \".nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog()\n",
    "ax.scatter(actual_dt, var)\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('Two sample phase variance')\n",
    "ax.set_title(f\"pulse_rep_int = {raw.attrs['config']['CHIRP']['pulse_rep_int']} s\")\n",
    "plt.grid()\n",
    "fig.savefig(output_base_2svar + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_phase = os.path.join(\"20230628-outputs/\", raw.attrs[\"basename\"]+\"-phase\")\n",
    "\n",
    "peak_idx_plot = peak_idxs.hvplot.scatter(x='pulse_idx')\n",
    "peak_phase_plot = peak_phases.hvplot.scatter(x='pulse_idx', datashade=True)\n",
    "peak_phase_rolling_plot = peak_phases.rolling(pulse_idx=100).mean().hvplot.scatter(x='pulse_idx', datashade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.save(peak_idx_plot, output_base_phase+\"-peak-idx.png\", fmt='png')\n",
    "hv.save(peak_phase_plot, output_base_phase+\"-peak-phase.png\", fmt='png')\n",
    "hv.save(peak_phase_rolling_plot, output_base_phase+\"-peak-phase-rolling.png\", fmt='png')\n",
    "\n",
    "hv.save(peak_idx_plot, output_base_phase+\"-peak-idx.html\", fmt='widgets')\n",
    "hv.save(peak_phase_plot, output_base_phase+\"-peak-phase.html\", fmt='widgets')\n",
    "hv.save(peak_phase_rolling_plot, output_base_phase+\"-peak-phase-rolling.html\", fmt='widgets')\n",
    "\n",
    "peak_idx_plot, peak_phase_plot, peak_phase_rolling_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rg2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
